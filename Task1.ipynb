{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Data Preprocessing Pipeline using Pandas and Scikit-learn\n"
      ],
      "metadata": {
        "id": "FVnv5n9bOIVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "Create an ETL pipeline for preprocessing a dataset using pandas and scikit-learn tools like:\n",
        "- SimpleImputer\n",
        "- OneHotEncoder\n",
        "- StandardScaler\n",
        "- ColumnTransformer\n",
        "- Pipeline"
      ],
      "metadata": {
        "id": "hC03o38UOOiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required *Libraries*"
      ],
      "metadata": {
        "id": "YCyKnwu6OTSd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xk8TOn5KEl4K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset\n",
        "We're using the Titanic dataset from OpenML for demonstration.\n"
      ],
      "metadata": {
        "id": "Hz3QOkAGOf6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dataset\n",
        "\n",
        "# data = pd.read_csv(\"filename.csv\")\n",
        "data = fetch_openml(name='titanic', version=1, as_frame=True)['frame']"
      ],
      "metadata": {
        "id": "47veMkvZF-O4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Separate features and target\n",
        "\n",
        "X = data.drop(columns=['survived'])\n",
        "y = data['survived']"
      ],
      "metadata": {
        "id": "qr5oscvHGCcs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the Feature Types\n",
        "We separate features into numeric and categorical to apply suitable preprocessing steps.\n"
      ],
      "metadata": {
        "id": "4hmLWF_gOzBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Identify column types\n",
        "\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns"
      ],
      "metadata": {
        "id": "irg6ipvUGFhR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Numeric and Categorical Pipelines\n"
      ],
      "metadata": {
        "id": "NkWzSdAqO5y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define transformers\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])"
      ],
      "metadata": {
        "id": "J4QeAIS3GI4R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Combine transformers\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "metadata": {
        "id": "rNn9XVofGRCO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Create a complete pipeline\n",
        "\n",
        "etl_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])"
      ],
      "metadata": {
        "id": "kX3cGisPGTK9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Apply pipeline to training data\n",
        "\n",
        "X_processed = etl_pipeline.fit_transform(X)"
      ],
      "metadata": {
        "id": "y9acvpeeGVFg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Preprocessing and Split the Data"
      ],
      "metadata": {
        "id": "PHdeFOmnPA2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Train-test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"ETL pipeline completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TZabh8mGXnl",
        "outputId": "044887be-1b55-4f9e-ee79-1dee7e054977"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETL pipeline completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "We have successfully created a reusable ETL pipeline using Scikit-learn to:\n",
        "- Handle missing values\n",
        "- Scale numeric features\n",
        "- Encode categorical features\n",
        "- Prepare clean training and testing sets\n"
      ],
      "metadata": {
        "id": "m_PYfnSYPP-_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7iQk0zIGYc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}